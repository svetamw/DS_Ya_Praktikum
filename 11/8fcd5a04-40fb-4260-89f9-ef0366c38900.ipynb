{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек и загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import notebook\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toxic = pd.read_csv('datasets/toxic_comments.csv')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n",
      "0    143346\n",
      "1     16225\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_toxic.info())\n",
    "print(df_toxic['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_toxic['text'].astype('U').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим данные от небуквенных символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    new_text = re.sub(r'[^a-z]',' ',text)\n",
    "    split_text = new_text.split()\n",
    "    return ' '.join(split_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_data = []\n",
    "for i in range(len(corpus)): \n",
    "    clean_text = clear_text(corpus[i])\n",
    "    tokens = nltk.word_tokenize(clean_text)\n",
    "    stemmer = EnglishStemmer(ignore_stopwords=False)\n",
    "    lemm = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "    lemm_data.append(lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим пример, как выглядит текст до лемматизации и после"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:\n",
      "explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n",
      "\n",
      "AFTER:\n",
      "explan whi the edit made under my usernam hardcor metallica fan were revert they weren t vandal just closur on some gas after i vote at new york doll fac and pleas don t remov the templat from the talk page sinc i m retir now\n"
     ]
    }
   ],
   "source": [
    "print('BEFORE:')\n",
    "print(corpus[0])\n",
    "print()\n",
    "print('AFTER:')\n",
    "print(lemm_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_toxic['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_data_train, lemm_data_test, target_train, target_test = train_test_split(\n",
    "    lemm_data, target, test_size = 0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    114670\n",
      "1     12986\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(target_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer()\n",
    "tf_idf_tr = count_tf_idf.fit_transform(lemm_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_te = count_tf_idf.transform(lemm_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = tf_idf_tr\n",
    "features_test = tf_idf_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В ходе подготовки текста комментариев к обучению модели, была проведена его предобработка.\n",
    "\n",
    "Убраны лишние (небуквенные) символы, проведена лемматизация.\n",
    "\n",
    "Данные разбиты на обучающую и тестовую выборки.\n",
    "\n",
    "Признаки векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список `rows`, чтобы сохранять результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "gs = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid=grid,\n",
    "                  scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: {'C': 10.0, 'penalty': 'l2'} f1_score: 0.7695742665949652\n"
     ]
    }
   ],
   "source": [
    "gs.fit(features_train, target_train)\n",
    "print('param:', gs.best_params_, 'f1_score:', gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = {'model':'LogisticRegression', \n",
    "      'f1_score': gs.best_score_}\n",
    "rows.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha':[1, 10, 200, 265, 270, 275, 500],\n",
    "               'penalty':[\"l1\",\"l2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 =  GridSearchCV(SGDClassifier(class_weight='balanced'),\n",
    "                    param_grid=params,\n",
    "                    scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: {'alpha': 1, 'penalty': 'l2'} f1_score: 0.20403773989438892\n"
     ]
    }
   ],
   "source": [
    "gs2.fit(features_train, target_train)\n",
    "print('param:', gs2.best_params_, 'f1_score:', gs2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = {'model':'SGDClassifier', 'f1_score': gs2.best_score_}\n",
    "rows.append(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_params = {'alpha':[1, 10, 200, 265, 270, 275, 500]}\n",
    "\n",
    "gs3 =  GridSearchCV(RidgeClassifier(class_weight='balanced'), \n",
    "                    param_grid=ridge_params, \n",
    "                    scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: {'alpha': 10} f1_score: 0.7062555282064159\n"
     ]
    }
   ],
   "source": [
    "gs3.fit(features_train, target_train)\n",
    "print('param:', gs3.best_params_, 'f1_score:', gs3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = {'model':'RidgeClassifier', 'f1_score': gs3.best_score_}\n",
    "rows.append(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model  f1_score\n",
      "0  LogisticRegression  0.769574\n",
      "1     RidgeClassifier  0.706256\n",
      "2       SGDClassifier  0.204038\n"
     ]
    }
   ],
   "source": [
    "table_f1 = pd.DataFrame(rows)\n",
    "print(table_f1.sort_values(by='f1_score', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном разделе было обучено 3 модели: логистическая регрессия, sgd классификатор, основанный на градиентном спуске и гребневая регрессия (Ридж).\n",
    "\n",
    "Лучший результат показала логистическая регрессия. Этот результат является удовлетворительным, т.к. выше необходимого показателя (0.75)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "значение f1 на тестовой выборке: 0.7686230248306998\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced', C=10.0, penalty='l2')\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "f1_tfidf_te = f1_score(target_test, predictions)\n",
    "print('значение f1 на тестовой выборке:', f1_tfidf_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Проведена предобработка данных: лемматизация и векторизация\n",
    "- Обучено 3 модели с подбором параметров по лучшей f1_score\n",
    "- Выбрана линейная регрессия, для которой f1_score > 0.75\n",
    "\n",
    "Итог работы: Модель сможет классифицировть комментарии на позитивный и негативный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
